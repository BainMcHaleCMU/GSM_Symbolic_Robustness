# -*- coding: utf-8 -*-
"""testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18faQXsB3nFYeBkh4fpHsU2V3PeNxzMnx
"""

!pip install -q -U transformers bitsandbytes

# #connect to drive
# from google.colab import drive
# drive.mount('/content/drive')

# %cd /content/drive/MyDrive/sem 3 code/GSM_Symbolic_Robustness-main/dataset

# !unzip GSM_Symbolic_Robustness-main.zip

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/GSM_Symbolic_Robustness/dataset

config = {
    'batch_size': 1,
    'model_name': "google/gemma-2-2b-it",
    'tokenizer_name': 'google/gemma-2-2b-it',
    'token': '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',
    'is_mistral_or_llama': False,
    'output_name': 'test_output',
    'val_dataset_path': 'gsm_symbolic/data/noop_v1.jsonl',
    'use_calculator': True,
    'max_calculator_regenerations': 3,
}

# import importlib
# importlib.reload(dataset_and_dataloader)

import dataset_and_dataloader
from transformers import AutoTokenizer
from transformers import BitsAndBytesConfig
from transformers import AutoModelForCausalLM
import torch
import re
import numpy as np
from tqdm import tqdm

tokenizer = AutoTokenizer.from_pretrained(config['tokenizer_name'], token = config['token'])
if config['is_mistral_or_llama']:
  tokenizer.add_special_tokens({'pad_token': '[PAD]'})
val_dataset = dataset_and_dataloader.GSM8K_Val_Dataset(config['val_dataset_path'], tokenizer)
val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=True)

# load in quantization config
q_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True
)

model_name = config["model_name"]

model = AutoModelForCausalLM.from_pretrained(model_name, token=config['token'], quantization_config=q_config)
if config['is_mistral_or_llama']:
  model.config.pad_token_id = tokenizer.pad_token_id

if torch.backends.mps.is_available():
    device = torch.device("mps")
else:
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"device: {device}")

model.to(device)
model.train()

def extract_a(text):
  regex = re.compile(r"#### (\-?[0-9\.\,]+)")
  model_answer = regex.search(text)
  if model_answer:
    answer = model_answer.group(1)
    answer = answer.replace(",", "")
    answer = answer.replace(" ", "")
    return answer
  else:
    return "No Valid Answer Found"

def get_accuracy(outputs, a):
  pred_a = [extract_a(out) for out in outputs]
  a = [extract_a(ans) for ans in a]
  error_rate = np.mean(np.array(pred_a) == "No Valid Answer Found")
  return np.mean(np.array(pred_a) == np.array(a)), error_rate

calculator_regex = re.compile(r"<<([0-9\.\, \*\+\-=/\(\)]*)>>")

acc = 0
error_rate = 0
pbar = tqdm(val_dataloader)
num_batches_done = 0
to_write = dict()
for q_id, q, q_mask, a in pbar:
    # move to device
    q = torch.stack(q, dim=1).to(device)
    q_mask = torch.stack(q_mask, dim=1).to(device)
    # do inference
    pred_a = model.generate(
      input_ids=q,
      attention_mask=q_mask,
      max_new_tokens=256
    )
    outputs = tokenizer.batch_decode(pred_a[:, q.shape[-1]:], skip_special_tokens=True)

    if config['use_calculator']:
      # CALCULATOR!
      error_counter = 0
      i = 0
      while i < len(outputs) and error_counter < config['max_calculator_regenerations']:
        if "<<" in outputs[i]:
          check_output = calculator_regex.findall(outputs[i])
          for equation in check_output:
            equation = equation.replace("=", "==")
            if not eval(equation):
              print("REGENERATING: wrong equation -> "+ equation)
              pred_a = model.generate(
                input_ids=q,
                attention_mask=q_mask,
                max_new_tokens=256
              )
              outputs = tokenizer.batch_decode(pred_a[:, q.shape[-1]:], skip_special_tokens=True)
              error_counter += 1
              i = -1
              break
        i += 1

    to_write.update(dict(zip(q_id, outputs)))
    new_acc, new_error_rate = get_accuracy(outputs, a)
    acc += new_acc
    error_rate += new_error_rate
    num_batches_done += 1
    # do display
    pbar.set_postfix(accuracy=f"{(acc/num_batches_done):.2f}%", loss=f"{(error_rate/num_batches_done):.2f}")
    pbar.update(1)
acc /= len(val_dataloader)
error_rate /= len(val_dataloader)
print(f"\nAccuracy: {acc}")
print(f"Error Rate: {error_rate}")

with open("outputs/"+config['output_name']+"_acc="+str(acc)+"_error="+str(error_rate)+".txt", "w") as f:
  f.write(str(to_write))